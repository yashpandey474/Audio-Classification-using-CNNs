{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtUcBE4M7Pc7",
        "outputId": "aadfaf3a-f67c-4c88-ca4c-1257504fdd3d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "import librosa\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# FILEPATH: /Users/kmpandey/Desktop/3-2/Deep Learning/Project/Notebooks/Project_1 (1).ipyn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting librosa\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa)\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (1.26.4)\n",
            "Collecting scipy>=1.2.0 (from librosa)\n",
            "  Downloading scipy-1.12.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.9/217.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=0.20.0 (from librosa)\n",
            "  Downloading scikit_learn-1.4.1.post1-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
            "Collecting joblib>=0.14 (from librosa)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /Users/kmpandey/Library/Python/3.12/lib/python/site-packages (from librosa) (5.1.1)\n",
            "Collecting numba>=0.51.0 (from librosa)\n",
            "  Downloading numba-0.59.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
            "Collecting soundfile>=0.12.1 (from librosa)\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (14 kB)\n",
            "Collecting pooch>=1.0 (from librosa)\n",
            "  Downloading pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting soxr>=0.3.2 (from librosa)\n",
            "  Downloading soxr-0.3.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (4.10.0)\n",
            "Collecting lazy-loader>=0.1 (from librosa)\n",
            "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting msgpack>=1.0 (from librosa)\n",
            "  Downloading msgpack-1.0.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba>=0.51.0->librosa)\n",
            "  Downloading llvmlite-0.42.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /Users/kmpandey/Library/Python/3.12/lib/python/site-packages (from pooch>=1.0->librosa) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/kmpandey/Library/Python/3.12/lib/python/site-packages (from pooch>=1.0->librosa) (24.0)\n",
            "Collecting requests>=2.19.0 (from pooch>=1.0->librosa)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->librosa)\n",
            "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa)\n",
            "  Using cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
            "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa)\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->pooch>=1.0->librosa)\n",
            "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.19.0->pooch>=1.0->librosa)\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch>=1.0->librosa)\n",
            "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n",
            "Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
            "Downloading msgpack-1.0.8-cp312-cp312-macosx_11_0_arm64.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.59.0-cp312-cp312-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pooch-1.8.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp312-cp312-macosx_12_0_arm64.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp312-cp312-macosx_12_0_arm64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading soundfile-0.12.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading soxr-0.3.7-cp312-cp312-macosx_11_0_arm64.whl (390 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached cffi-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (177 kB)\n",
            "Downloading llvmlite-0.42.0-cp312-cp312-macosx_11_0_arm64.whl (28.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
            "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Installing collected packages: urllib3, threadpoolctl, soxr, scipy, pycparser, msgpack, llvmlite, lazy-loader, joblib, idna, charset-normalizer, audioread, scikit-learn, requests, numba, cffi, soundfile, pooch, librosa\n",
            "Successfully installed audioread-3.0.1 cffi-1.16.0 charset-normalizer-3.3.2 idna-3.6 joblib-1.3.2 lazy-loader-0.3 librosa-0.10.1 llvmlite-0.42.0 msgpack-1.0.8 numba-0.59.0 pooch-1.8.1 pycparser-2.21 requests-2.31.0 scikit-learn-1.4.1.post1 scipy-1.12.0 soundfile-0.12.1 soxr-0.3.7 threadpoolctl-3.3.0 urllib3-2.2.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract files from zip folder [ONLY EXECUTE ONCE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the zip file\n",
        "zip_file_path = '/content/drive/My Drive/audio_dataset.zip'\n",
        "\n",
        "# Directory to extract the contents\n",
        "extract_dir = '/content/drive/My Drive/audio_dataset'\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get all class names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/My Drive/audio_dataset/audio_dataset/train'\n",
        "classes = os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View info about duration of clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_duration_info(data):\n",
        "    duration_df = pd.DataFrame(columns=['Duration'])\n",
        "\n",
        "    # Iterate over each waveform in the list and get its duration\n",
        "    for i, waveform in enumerate(train_data):\n",
        "        duration = librosa.get_duration(y=waveform)\n",
        "        duration_df.loc[i] = duration\n",
        "\n",
        "    # Use .describe() to get summary statistics\n",
        "    summary_stats = duration_df.describe()\n",
        "\n",
        "    print(summary_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameters for creation of mel spectograms & model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "duration_seconds = 4\n",
        "sample_rate = 22050\n",
        "hyper_params = {\n",
        "    'duration': duration_seconds*sample_rate,\n",
        "     'n_mels': 128,\n",
        "    'hop_length': 512,\n",
        "    'n_fft': 2048,\n",
        "    'fmin': 20\n",
        "}\n",
        "\n",
        "model_params = {\n",
        "    'num_epochs': 50,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'num_clases': 10,\n",
        "    'eta_min': 1e-5,\n",
        "    't_max': 10\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for loading data & create mel spectograms, displaying audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio(file_path):\n",
        "  waveform, sample_rate = librosa.load(file_path, sr=None)\n",
        "  waveform, index = librosa.effects.trim(waveform, top_db=60)\n",
        "\n",
        "    # pad to a length of 4s\n",
        "  if len(waveform) > hyper_params['duration']:\n",
        "      waveform = waveform[:hyper_params['duration']]\n",
        "  else:\n",
        "      padding = hyper_params['duration'] - len(waveform)\n",
        "      offset = padding // 2\n",
        "      waveform = np.pad(waveform, (offset, hyper_params['duration'] - len(waveform) - offset), 'constant')\n",
        "\n",
        "  return waveform, sample_rate\n",
        "\n",
        "def create_melspec(params, audio_data, sampling_rate):\n",
        "  S = librosa.feature.melspectrogram(audio_data,\n",
        "                                       sr=sampling_rate,\n",
        "                                       n_mels=params['n_mels'],\n",
        "                                       hop_length=params['hop_length'],\n",
        "                                       n_fft=params['n_fft'],\n",
        "                                       fmin=params['fmin'],\n",
        "                                       fmax=(sampling_rate // 2))\n",
        "  Sb = librosa.power_to_db(S, ref=np.max)\n",
        "  Sb = Sb.astype(np.float32)\n",
        "\n",
        "  return Sb\n",
        "\n",
        "def load_data(data_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            if file_name.endswith('.wav'):\n",
        "                file_path = os.path.join(class_dir, file_name)\n",
        "                waveform, sample_rate = load_audio(file_path)\n",
        "\n",
        "                data.append(waveform)\n",
        "                labels.append(class_name)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "def display_audio(audio_file_path):\n",
        "  waveform, sample_rate = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "  # Plot the waveform\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  librosa.display.waveshow(waveform, sr=sample_rate)\n",
        "  plt.xlabel('Time (s)')\n",
        "  plt.ylabel('Amplitude')\n",
        "  plt.title('Waveform of Audio File')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#GET WAVEFORMS IN A LIST\n",
        "train_data, train_labels = load_data('/content/drive/My Drive/audio_dataset/audio_dataset/train')\n",
        "\n",
        "#GET [VAL] WAVEFORMS IN A LIST\n",
        "val_data, val_labels = load_data('/content/drive/My Drive/audio_dataset/audio_dataset/val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6nfKijX-IHJ"
      },
      "outputs": [],
      "source": [
        "class CNNModel1(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 10, kernel_size = 5, stride = 1)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 10, out_channels = 10, kernel_size = 5, stride = 1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    self.lin1 = nn.Linear(in_features= 4*4*10,out_features = 100)\n",
        "    self.lin2 = nn.Linear(100,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.relu(self.lin1(x))\n",
        "    x = self.lin2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGM3J2RwCcCy"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.avg_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvBlock(in_channels=3, out_channels=64),\n",
        "            ConvBlock(in_channels=64, out_channels=128),\n",
        "            ConvBlock(in_channels=128, out_channels=256),\n",
        "            ConvBlock(in_channels=256, out_channels=512),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.PReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = torch.mean(x, dim=3)\n",
        "        x, _ = torch.max(x, dim=2)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUGJ-pA7_WlZ"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(predictions, targets):\n",
        "  length = len(predictions)\n",
        "  correct = 0\n",
        "  for idx in range(length):\n",
        "    if predictions[idx] == targets[idx]: correct +=1\n",
        "\n",
        "  return (correct/length) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfBHXhxz-IJs"
      },
      "outputs": [],
      "source": [
        "train_x = train_x.unsqueeze(1)\n",
        "validation_x = validation_x.unsqueeze(1)\n",
        "test_x = test_x.unsqueeze(1) # unsqueezing to introduce batchsize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gORuthcO_qxc"
      },
      "outputs": [],
      "source": [
        "model1 = CNNModel1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1Y2CQzR-IMC"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "num_batches_per_train_epoch = train_x.shape[0] // batch_size\n",
        "num_batches_validation = validation_x.shape[0] // batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ITCy4g-IOR"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liWEUyGs_1tx"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_x, train_y, validation_x, validation_y):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  train_preds = list()\n",
        "  train_targets = list()\n",
        "\n",
        "\n",
        "    # Train one epoch\n",
        "  for batch_idx in range(num_batches_per_train_epoch):\n",
        "    optimizer.zero_grad()  # This line is necessary to flush out the gradients of the previous batch.\n",
        "\n",
        "    input = train_x[batch_idx*batch_size: (batch_idx+1)*batch_size] # Slice out batch_size amount of the training data\n",
        "    output = model(input)\n",
        "    target_out = train_y[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "    preds = torch.argmax(output, dim=1)\n",
        "\n",
        "    train_preds +=(list(preds.detach().cpu().numpy()))\n",
        "    train_targets+=(list(target_out.detach().cpu().numpy()))\n",
        "    batch_loss = criterion(output, target_out)\n",
        "\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += batch_loss\n",
        "  epoch_loss_ret = epoch_loss.detach().cpu() / batch_size\n",
        "\n",
        "    # Switch model to eval mode since we do not want to update our weights using test/val set images! They are for measuring performance only\n",
        "  model.eval()\n",
        "    # Training Performance at the end of epoch\n",
        "\n",
        "  val_preds = list()\n",
        "  val_targets = list()\n",
        "\n",
        "  for batch_idx in range(num_batches_validation):\n",
        "    input = validation_x[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "    output = model(input)\n",
        "    target_out = validation_y[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "    # preds = torch.argmax(output, dim=1)\n",
        "    preds = torch.max(output, 1)[1]\n",
        "\n",
        "\n",
        "    val_preds += (list(preds.detach().cpu().numpy()))\n",
        "    val_targets+=(list(target_out.detach().cpu().numpy()))\n",
        "\n",
        "  train_accuracy = get_accuracy(train_preds, train_targets)\n",
        "  val_accuracy = get_accuracy(val_preds, val_targets)\n",
        "\n",
        "  return train_accuracy, val_accuracy, epoch_loss_ret\n",
        "\n",
        "def train_model(train_x, train_y, validation_x, validation_y, model, num_epochs):\n",
        "    # Train the model\n",
        "  epoch_loss = 0\n",
        "  losses_at_each_epoch = list()\n",
        "  train_accuracies = list()\n",
        "  validation_accuracies = list()\n",
        "\n",
        "  # Forward pass -> Backward pass -> Weight update\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_accuracy, val_accuracy, epoch_loss = train_epoch(model, train_x, train_y, validation_x, validation_y)\n",
        "\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    validation_accuracies.append(val_accuracy)\n",
        "    losses_at_each_epoch.append(epoch_loss)\n",
        "\n",
        "    print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
        "                epoch, losses_at_each_epoch[-1], train_accuracies[-1], validation_accuracies[-1]))\n",
        "\n",
        "  return model, epoch_loss, losses_at_each_epoch, train_accuracies, validation_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBF6g59spbA2",
        "outputId": "c8431daa-c628-4d3d-b2a8-f04e70212fdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Duration\n",
            "count  4374.000000\n",
            "mean      5.866536\n",
            "std       3.327365\n",
            "min       0.120000\n",
            "25%       3.231746\n",
            "50%       5.549410\n",
            "75%       8.000000\n",
            "max      34.829932\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwCrVxzhp6iH"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.avg_pool2d(x, 2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEhAnEjaEMJy"
      },
      "outputs": [],
      "source": [
        "# RESNETS:\n",
        "class BasicBlock(nn.Module):\n",
        "\n",
        "  expansion=1 # expansion is 1 as there is no expansion factor is basic block\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1, bias=False) # 3x3 Conv Layer\n",
        "    self.bn1 = nn.BatchNorm2d(num_features = out_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    identity = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    x += identity\n",
        "\n",
        "    return (self.relu(x))\n",
        "\n",
        "\n",
        "class BottleNeckBlock(nn.Module):\n",
        "\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    base_width = 64\n",
        "\n",
        "    width = int(out_channels * (base_width / 64.)) * 1\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=width, kernel_size=1, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(num_features=width)\n",
        "    self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(num_features = width)\n",
        "    self.conv3 = nn.Conv2d(in_channels=width, out_channels=width * self.expansion , kernel_size=1, stride=stride, padding=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(num_features = width * self.expansion)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    identity = x\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "    x+= identity\n",
        "\n",
        "    return (self.relu(x))\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(residual)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, layers, num_classes):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = 64\n",
        "\n",
        "    # resnet stem\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(num_features = self.in_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    #res-blocks\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "    #classifier block\n",
        "    self.adppool = nn.AdaptiveAvgPool2d((2,2))\n",
        "    self.classifier = nn.Linear(in_features=512 * block.expansion, out_features = num_classes)\n",
        "\n",
        "  def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "\n",
        "    downsample = None\n",
        "\n",
        "    if stride!=1 or self.in_channels != out_channels * block.expansion:\n",
        "\n",
        "      downsample = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(num_features=out_channels * block.expansion)\n",
        "    )\n",
        "\n",
        "    layers=[]\n",
        "\n",
        "    layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "\n",
        "    self.in_channels = out_channels * block.expansion\n",
        "\n",
        "    for i in range(1, blocks):\n",
        "      layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.adppool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "\n",
        "    return self.classifier(x)\n",
        "\n",
        "\n",
        "class ExtendedResNetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.make_layer(64, 64, 3)\n",
        "        self.layer2 = self.make_layer(64, 128, 4, stride=2)\n",
        "        self.layer3 = self.make_layer(128, 256, 6, stride=2)\n",
        "        self.layer4 = self.make_layer(256, 512, 3, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(ResNetBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResNetBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDI4dVGjEML_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
