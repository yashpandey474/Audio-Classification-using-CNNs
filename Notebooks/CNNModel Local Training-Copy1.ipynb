{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0efb2ce-aed4-473d-bdd9-63fbef765c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import librosa.display\n",
    "import librosa\n",
    "import zipfile\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from PIL import Image\n",
    "import copy\n",
    "from collections import Counter\n",
    "import cv2\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce718db-1cda-460d-acb2-3081ecd431fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from melspecdataset import MelSpecDataset, normalize_by_255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b318dd-6958-4990-9acb-00ff10a83fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b7b3e7-a70a-493f-a225-9192dca4833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  # Optionally resize images\n",
    "    transforms.ToTensor(),            # Convert images to tensors\n",
    "    normalize_by_255        # Normalize by dividing by 255\n",
    "])\n",
    "\n",
    "\n",
    "# Define the mapping from class names to class indices\n",
    "class_mapping = {\n",
    "    'car_horn': 1,\n",
    "    'dog_barking': 2,\n",
    "    'drilling': 3,\n",
    "    'Fart': 4,\n",
    "    'Guitar': 5,\n",
    "    'Gunshot_and_gunfire': 6,\n",
    "    'Hi-hat': 7,\n",
    "    'Knock': 8,\n",
    "    'Laughter': 9,\n",
    "    'Shatter': 10,\n",
    "    'siren': 11,\n",
    "    'Snare_drum': 12,\n",
    "    'Splash_and_splatter': 13\n",
    "}\n",
    "\n",
    "# Define the directories\n",
    "train_directory = \"train\"\n",
    "val_directory = \"val\"\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MelSpecDataset(train_directory, class_mapping, transform)\n",
    "val_dataset = MelSpecDataset(val_directory, class_mapping, transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "datasets = {\"train\": train_dataset, \"val\": val_dataset}\n",
    "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812d7e33-27bb-4372-ad60-c5c243d8aff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Snare_drum': 1000,\n",
       "  'siren': 1000,\n",
       "  'Hi-hat': 1000,\n",
       "  'Gunshot_and_gunfire': 1000,\n",
       "  'car_horn': 1000,\n",
       "  'drilling': 1000,\n",
       "  'Guitar': 1000,\n",
       "  'Fart': 1000,\n",
       "  'Laughter': 1000,\n",
       "  'Splash_and_splatter': 1000,\n",
       "  'dog_barking': 1000,\n",
       "  'Shatter': 1000,\n",
       "  'Knock': 1000},\n",
       " 13000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_data, sum(train_dataset.class_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fe08a5-3b67-4485-b609-a2f194b7c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate input size for fully connected layers\n",
    "        self.fc_input_size = self._calculate_fc_input_size()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _calculate_fc_input_size(self):\n",
    "        # Calculate the size of the flattened output after convolution and pooling\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 3, self.input_shape[0], self.input_shape[1])  # Create dummy input tensor\n",
    "            x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "            x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "            x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "            return x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, self.fc_input_size)  # Flatten the output for fully connected layers\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6054ac53-aca9-4a6c-8f05-b24b485cfbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "  torch.save(model.state_dict(), f'{model_name}_weights.pth')\n",
    "  torch.save(model, f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0de5bf6-cde1-4195-9a0d-90d8d1fbff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        print(\"HELLO\")\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            index = 0\n",
    "            print(\"STARTING ITERATION\")\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "              print(\"BATCH NUMBER = \", index)\n",
    "              # inputs = inputs.to(device)\n",
    "              # labels = labels.to(device)\n",
    "\n",
    "              index += 1\n",
    "              optimizer.zero_grad()\n",
    "              with torch.set_grad_enabled(phase == 'train'):\n",
    "                  outputs = model(inputs)\n",
    "                  _, preds = torch.max(outputs, 1)\n",
    "                  loss = criterion(outputs, labels)\n",
    "\n",
    "                  if phase == 'train':\n",
    "                      loss.backward()\n",
    "                      optimizer.step()\n",
    "\n",
    "              running_loss += loss.item() * inputs.size(0)\n",
    "              running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "              print(\"STEPPING SCEHEDULER\")\n",
    "              scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'EPOCH: {epoch} {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model weights for the model which has the highest acc.\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7875ff1d-f6a7-4159-9829-6fba0ecc72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = (128, 345, 3)\n",
    "num_classes = 13  # Assuming 14 output classes\n",
    "\n",
    "# Instantiate the model\n",
    "model_ft = CNNModel(input_shape, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03835b5a-e64f-449c-a6a2-6ccdc59155a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9a949-0832-4e09-b872-75e58d801b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "HELLO\n",
      "STARTING ITERATION\n",
      "BATCH NUMBER =  0\n",
      "BATCH NUMBER =  1\n",
      "BATCH NUMBER =  2\n",
      "BATCH NUMBER =  3\n",
      "BATCH NUMBER =  4\n",
      "BATCH NUMBER =  5\n",
      "BATCH NUMBER =  6\n",
      "BATCH NUMBER =  7\n",
      "BATCH NUMBER =  8\n",
      "BATCH NUMBER =  9\n",
      "BATCH NUMBER =  10\n",
      "BATCH NUMBER =  11\n",
      "BATCH NUMBER =  12\n",
      "BATCH NUMBER =  13\n",
      "BATCH NUMBER =  14\n",
      "BATCH NUMBER =  15\n",
      "BATCH NUMBER =  16\n",
      "BATCH NUMBER =  17\n",
      "BATCH NUMBER =  18\n",
      "BATCH NUMBER =  19\n",
      "BATCH NUMBER =  20\n",
      "BATCH NUMBER =  21\n",
      "BATCH NUMBER =  22\n",
      "BATCH NUMBER =  23\n",
      "BATCH NUMBER =  24\n",
      "BATCH NUMBER =  25\n",
      "BATCH NUMBER =  26\n",
      "BATCH NUMBER =  27\n",
      "BATCH NUMBER =  28\n",
      "BATCH NUMBER =  29\n",
      "BATCH NUMBER =  30\n",
      "BATCH NUMBER =  31\n",
      "BATCH NUMBER =  32\n",
      "BATCH NUMBER =  33\n",
      "BATCH NUMBER =  34\n",
      "BATCH NUMBER =  35\n",
      "BATCH NUMBER =  36\n",
      "BATCH NUMBER =  37\n",
      "BATCH NUMBER =  38\n",
      "BATCH NUMBER =  39\n",
      "BATCH NUMBER =  40\n",
      "BATCH NUMBER =  41\n",
      "BATCH NUMBER =  42\n",
      "BATCH NUMBER =  43\n",
      "BATCH NUMBER =  44\n",
      "BATCH NUMBER =  45\n",
      "BATCH NUMBER =  46\n",
      "BATCH NUMBER =  47\n",
      "BATCH NUMBER =  48\n",
      "BATCH NUMBER =  49\n",
      "BATCH NUMBER =  50\n",
      "BATCH NUMBER =  51\n",
      "BATCH NUMBER =  52\n",
      "BATCH NUMBER =  53\n",
      "BATCH NUMBER =  54\n",
      "BATCH NUMBER =  55\n",
      "BATCH NUMBER =  56\n",
      "BATCH NUMBER =  57\n",
      "BATCH NUMBER =  58\n",
      "BATCH NUMBER =  59\n",
      "BATCH NUMBER =  60\n",
      "BATCH NUMBER =  61\n",
      "BATCH NUMBER =  62\n",
      "BATCH NUMBER =  63\n",
      "BATCH NUMBER =  64\n",
      "BATCH NUMBER =  65\n",
      "BATCH NUMBER =  66\n",
      "BATCH NUMBER =  67\n",
      "BATCH NUMBER =  68\n",
      "BATCH NUMBER =  69\n",
      "BATCH NUMBER =  70\n",
      "BATCH NUMBER =  71\n",
      "BATCH NUMBER =  72\n",
      "BATCH NUMBER =  73\n",
      "BATCH NUMBER =  74\n",
      "BATCH NUMBER =  75\n",
      "BATCH NUMBER =  76\n",
      "BATCH NUMBER =  77\n",
      "BATCH NUMBER =  78\n",
      "BATCH NUMBER =  79\n",
      "BATCH NUMBER =  80\n",
      "BATCH NUMBER =  81\n",
      "BATCH NUMBER =  82\n",
      "BATCH NUMBER =  83\n",
      "BATCH NUMBER =  84\n",
      "BATCH NUMBER =  85\n",
      "BATCH NUMBER =  86\n",
      "BATCH NUMBER =  87\n",
      "BATCH NUMBER =  88\n",
      "BATCH NUMBER =  89\n",
      "BATCH NUMBER =  90\n",
      "BATCH NUMBER =  91\n",
      "BATCH NUMBER =  92\n",
      "BATCH NUMBER =  93\n",
      "BATCH NUMBER =  94\n",
      "BATCH NUMBER =  95\n",
      "BATCH NUMBER =  96\n",
      "BATCH NUMBER =  97\n",
      "BATCH NUMBER =  98\n",
      "BATCH NUMBER =  99\n",
      "BATCH NUMBER =  100\n",
      "BATCH NUMBER =  101\n",
      "BATCH NUMBER =  102\n",
      "BATCH NUMBER =  103\n",
      "BATCH NUMBER =  104\n",
      "BATCH NUMBER =  105\n",
      "BATCH NUMBER =  106\n"
     ]
    }
   ],
   "source": [
    "# [SimplifiedResNet] AUGMENTED [& ENSURED VAL HAS BEEN PREPROCESSED]\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd10ab-aaaa-4a81-bd78-6beef137db23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
