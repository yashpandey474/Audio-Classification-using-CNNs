{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4ap-Bju4KIV"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z5HEs7OeAFkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a952a024-94c2-4340-a5bd-db92f4b14d76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "import librosa.display\n",
        "import librosa\n",
        "import zipfile\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import copy\n",
        "from collections import Counter\n",
        "import cv2\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZfEd92zAHcm",
        "outputId": "f9b0962c-4b51-4980-f7ad-7a482d8c917e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuJokiK97X57"
      },
      "outputs": [],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cb5m75-O__Xp"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    # transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class MelSpecDataset(Dataset):\n",
        "    def __init__(self, directory, class_mapping, transform):\n",
        "        self.directory = directory\n",
        "        self.class_mapping = class_mapping\n",
        "        self.data = []\n",
        "        self.class_data = {}\n",
        "        self.transform = transform\n",
        "\n",
        "        for class_name in os.listdir(directory):\n",
        "            class_dir = os.path.join(directory, class_name)\n",
        "            self.class_data[class_name] = 0\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "            class_label = self.class_mapping[class_name]  # Map class name to numerical label\n",
        "            for npz_file in os.listdir(class_dir):\n",
        "                npz_path = os.path.join(class_dir, npz_file)\n",
        "                self.data.append((npz_path, class_label))\n",
        "                self.class_data[class_name] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        npz_path, class_label = self.data[idx]\n",
        "        mel_spec = np.load(npz_path)['mel_spec']  # Assuming 'mel_spec' is the key for the mel spectrogram array\n",
        "        mel_spec = self.transform(mel_spec)\n",
        "        return mel_spec, class_label - 1\n",
        "\n",
        "# Define the mapping from class names to class indices\n",
        "class_mapping = {\n",
        "    'car_horn': 1,\n",
        "    'dog_barking': 2,\n",
        "    'drilling': 3,\n",
        "    'Fart': 4,\n",
        "    'Guitar': 5,\n",
        "    'Gunshot_and_gunfire': 6,\n",
        "    'Hi-hat': 7,\n",
        "    'Knock': 8,\n",
        "    'Laughter': 9,\n",
        "    'Shatter': 10,\n",
        "    'siren': 11,\n",
        "    'Snare_drum': 12,\n",
        "    'Splash_and_splatter': 13\n",
        "}\n",
        "\n",
        "# Define the directories\n",
        "train_directory = \"/content/drive/My Drive/DLproject-Numpy/train\"\n",
        "val_directory = \"/content/drive/My Drive/DLproject-Numpy/val\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MelSpecDataset(train_directory, class_mapping, transform)\n",
        "val_dataset = MelSpecDataset(val_directory, class_mapping, transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jrGxC1_3FDP1"
      },
      "outputs": [],
      "source": [
        "datasets = {\"train\": train_dataset, \"val\": val_dataset}\n",
        "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_jVr7LQT42m",
        "outputId": "e7d35628-dbad-41fc-822d-301aa083552f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Fart': 1000,\n",
              "  'Guitar': 1000,\n",
              "  'Gunshot_and_gunfire': 1000,\n",
              "  'Hi-hat': 1000,\n",
              "  'Knock': 1000,\n",
              "  'Laughter': 1000,\n",
              "  'Shatter': 1000,\n",
              "  'Snare_drum': 1000,\n",
              "  'Splash_and_splatter': 1000,\n",
              "  'car_horn': 1000,\n",
              "  'dog_barking': 1000,\n",
              "  'drilling': 1000,\n",
              "  'siren': 1000},\n",
              " 13000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_dataset.class_data, sum(train_dataset.class_data.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_ht4TzoFShV",
        "outputId": "0b3d1561-9f48-42b5-d273-b739f4011df3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 13000, 'val': 1209}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgSeAK31zUzX"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate input size for fully connected layers\n",
        "        self.fc_input_size = self._calculate_fc_input_size()\n",
        "\n",
        "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _calculate_fc_input_size(self):\n",
        "        # Calculate the size of the flattened output after convolution and pooling\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, self.input_shape[0], self.input_shape[1])  # Create dummy input tensor\n",
        "            x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "            x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "            x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "            return x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, self.fc_input_size)  # Flatten the output for fully connected layers\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Calculate input size for the first fully connected layer\n",
        "        self.fc_input_size = self._calculate_fc_input_size(input_shape)\n",
        "\n",
        "        self.rnn = nn.LSTM(256 * (input_shape[1]//8) * (input_shape[2]//8), 256, batch_first=True)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1, 256 * (x.size(2)//8) * (x.size(3)//8))\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "    def _calculate_fc_input_size(self, input_shape):\n",
        "        # Helper function to calculate the input size for the fully connected layer\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, input_shape[0], input_shape[1])\n",
        "            x = self.conv1(x)\n",
        "            x = self.pool(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.pool(x)\n",
        "            x = self.conv3(x)\n",
        "            x = self.pool(x)\n",
        "            return x.view(1, -1).shape[1]\n"
      ],
      "metadata": {
        "id": "43cAAGUbXmSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGish(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGish, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.fc1 = nn.Linear(512 * 2 * 3, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5nV-bPo5XplD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes=14):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Calculate input size for the first fully connected layer based on input_shape\n",
        "        self.fc_input_size = self._calculate_fc_input_size(input_shape)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_size, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def _calculate_fc_input_size(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, input_shape[0], input_shape[1])\n",
        "            x = self.features(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            return x.size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wu0cJrbEXxqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel2(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(CNNModel2, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "\n",
        "        # Calculate input size for fully connected layers\n",
        "        self.fc_input_size = self._calculate_fc_input_size()\n",
        "\n",
        "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _calculate_fc_input_size(self):\n",
        "        # Calculate the size of the flattened output after convolution and pooling\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, self.input_shape[0], self.input_shape[1])  # Create dummy input tensor\n",
        "            x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "            x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "            x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "            return x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, self.fc_input_size)  # Flatten the output for fully connected layers\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vV7iJ7JA0yza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VGG10(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(VGG10, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Calculate input size for fully connected layers\n",
        "        self.fc_input_size = self._calculate_fc_input_size()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_size, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def _calculate_fc_input_size(self):\n",
        "        # Calculate the size of the flattened output after convolution and pooling\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, self.input_shape[0], self.input_shape[1])  # Create dummy input tensor\n",
        "            x = self.features(x)\n",
        "            return x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.fc_input_size)  # Flatten the output for fully connected layers\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Hiq843eP5zKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveNetAudio(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes, num_layers=10, kernel_size=3, dilation=2):\n",
        "        super(WaveNetAudio, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            dilation_factor = dilation ** i\n",
        "            padding = dilation_factor * (kernel_size - 1) // 2\n",
        "            conv_layer = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=kernel_size, padding=padding, dilation=dilation_factor)\n",
        "            self.conv_layers.append(conv_layer)\n",
        "\n",
        "        # Calculate input size for fully connected layers\n",
        "        self.fc_input_size = self._calculate_fc_input_size(input_shape)\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_size, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def _calculate_fc_input_size(self, input_shape):\n",
        "        # Calculate the size of the flattened output after convolution\n",
        "        x = torch.zeros(1, 3, input_shape[0], input_shape[1])\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "        return x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "        x = x.view(-1, self.fc_input_size)  # Flatten the output for fully connected layers\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dwIPtvU5Sxm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio=1):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "        self.expand_ratio = expand_ratio\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride=stride, padding=kernel_size // 2,\n",
        "                               groups=hidden_dim, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
        "        self.conv3 = nn.Conv2d(hidden_dim, out_channels, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride == 1 and in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x += self.shortcut(identity)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes=13, width_mult=1.0):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfgs = [\n",
        "            # expand_ratio, channels, repeats, stride, kernel_size\n",
        "            [1, 16, 1, 1, 3],\n",
        "            [6, 24, 2, 2, 3],\n",
        "            [6, 40, 2, 2, 5],\n",
        "            [6, 80, 3, 2, 3],\n",
        "            [6, 112, 3, 1, 5],\n",
        "            [6, 192, 4, 2, 5],\n",
        "            [6, 320, 1, 1, 3]\n",
        "        ]\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.features = []\n",
        "        input_channel = 32\n",
        "        # building first layer\n",
        "        output_channel = int(32 * width_mult)\n",
        "        self.features.append(nn.Conv2d(3, output_channel, kernel_size=3, stride=2, padding=1, bias=False))\n",
        "        self.features.append(nn.BatchNorm2d(output_channel))\n",
        "        self.features.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # building inverted residual blocks\n",
        "        for expand_ratio, channels, repeats, stride, kernel_size in self.cfgs:\n",
        "            output_channel = int(channels * width_mult)\n",
        "            for i in range(repeats):\n",
        "                if i == 0:\n",
        "                    self.features.append(MBConvBlock(input_channel, output_channel, kernel_size, stride, expand_ratio))\n",
        "                else:\n",
        "                    self.features.append(MBConvBlock(input_channel, output_channel, kernel_size, 1, expand_ratio))\n",
        "                input_channel = output_channel\n",
        "\n",
        "        # building last several layers\n",
        "        self.features.append(nn.AdaptiveAvgPool2d(1))\n",
        "        self.features.append(nn.Flatten())\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        self.classifier = nn.Linear(int(1280 * width_mult), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# input_shape = (3, 224, 224)\n",
        "# num_classes = 10\n",
        "# model = EfficientNet(input_shape=input_shape, num_classes=num_classes, width_mult=1.0)\n",
        "# print(model)\n"
      ],
      "metadata": {
        "id": "CQByC7zKIOH2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NUUD_dWUDNIg"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        print(\"HELLO\")\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            index = 0\n",
        "            print(\"STARTING ITERATION\")\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "              print(\"BATCH NUMBER = \", index)\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              index += 1\n",
        "              optimizer.zero_grad()\n",
        "              with torch.set_grad_enabled(phase == 'train'):\n",
        "                  outputs = model(inputs)\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "                  loss = criterion(outputs, labels)\n",
        "\n",
        "                  if phase == 'train':\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "              running_loss += loss.item() * inputs.size(0)\n",
        "              running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "              print(\"STEPPING SCEHEDULER\")\n",
        "              scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'EPOCH: {epoch} {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model weights for the model which has the highest acc.\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k2OKssPqDOve"
      },
      "outputs": [],
      "source": [
        "# Define input shape and number of classes\n",
        "input_shape = (128, 345, 3)\n",
        "num_classes = 13  # Assuming 14 output classes\n",
        "\n",
        "# Instantiate the model\n",
        "model_ft = EfficientNet(input_shape, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_name):\n",
        "  torch.save(model.state_dict(), f'{model_name}_weights.pth')\n",
        "  torch.save(model, f'{model_name}.pth')"
      ],
      "metadata": {
        "id": "WbZzHb2UYc6T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = model_ft.to(device)"
      ],
      "metadata": {
        "id": "bHN4VL-Lhb3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eeSH2iZU3771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "736c29d6-5329-4771-fc03-355d07d99eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/6\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (87) must match the size of tensor b (173) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0a3fa63039fc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# [VGG16] AUGMENTED [& ENSURED VAL HAS BEEN PREPROCESSED]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0m\u001b[1;32m      3\u001b[0m                        num_epochs=7)\n",
            "\u001b[0;32m<ipython-input-6-23045af99cf3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m               \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5265466c7dd9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5265466c7dd9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (87) must match the size of tensor b (173) at non-singleton dimension 3"
          ]
        }
      ],
      "source": [
        "# [EFFICIENT-NET] AUGMENTED [& ENSURED VAL HAS BEEN PREPROCESSED]\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model_ft, \"VGG16\")"
      ],
      "metadata": {
        "id": "8FiED7RoYf85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-54SS412CV0",
        "outputId": "6a21f37a-0528-44cd-d0c0-8ccf91b8859e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 1.7684 Acc: 0.5111\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.9090 Acc: 0.7014\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 0.5614 Acc: 0.8203\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.6238 Acc: 0.8081\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 0.2630 Acc: 0.9179\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.5134 Acc: 0.8586\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 0.1419 Acc: 0.9589\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.5616 Acc: 0.8478\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 0.0901 Acc: 0.9741\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.5206 Acc: 0.8768\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 0.0688 Acc: 0.9817\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.5511 Acc: 0.8701\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n"
          ]
        }
      ],
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcJc-UEWDZsc",
        "outputId": "14767b46-b5a0-4dc7-8835-67ba247c861e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/1\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 1.3471 Acc: 0.5587\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.9327 Acc: 0.7080\n",
            "\n",
            "Epoch 1/1\n",
            "----------\n",
            "HELLO\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "BATCH NUMBER =  19\n",
            "BATCH NUMBER =  20\n",
            "BATCH NUMBER =  21\n",
            "BATCH NUMBER =  22\n",
            "BATCH NUMBER =  23\n",
            "BATCH NUMBER =  24\n",
            "BATCH NUMBER =  25\n",
            "BATCH NUMBER =  26\n",
            "BATCH NUMBER =  27\n",
            "BATCH NUMBER =  28\n",
            "BATCH NUMBER =  29\n",
            "BATCH NUMBER =  30\n",
            "BATCH NUMBER =  31\n",
            "BATCH NUMBER =  32\n",
            "BATCH NUMBER =  33\n",
            "BATCH NUMBER =  34\n",
            "BATCH NUMBER =  35\n",
            "BATCH NUMBER =  36\n",
            "BATCH NUMBER =  37\n",
            "BATCH NUMBER =  38\n",
            "BATCH NUMBER =  39\n",
            "BATCH NUMBER =  40\n",
            "BATCH NUMBER =  41\n",
            "BATCH NUMBER =  42\n",
            "BATCH NUMBER =  43\n",
            "BATCH NUMBER =  44\n",
            "BATCH NUMBER =  45\n",
            "BATCH NUMBER =  46\n",
            "BATCH NUMBER =  47\n",
            "BATCH NUMBER =  48\n",
            "BATCH NUMBER =  49\n",
            "BATCH NUMBER =  50\n",
            "BATCH NUMBER =  51\n",
            "BATCH NUMBER =  52\n",
            "BATCH NUMBER =  53\n",
            "BATCH NUMBER =  54\n",
            "BATCH NUMBER =  55\n",
            "BATCH NUMBER =  56\n",
            "BATCH NUMBER =  57\n",
            "BATCH NUMBER =  58\n",
            "BATCH NUMBER =  59\n",
            "BATCH NUMBER =  60\n",
            "BATCH NUMBER =  61\n",
            "BATCH NUMBER =  62\n",
            "BATCH NUMBER =  63\n",
            "BATCH NUMBER =  64\n",
            "BATCH NUMBER =  65\n",
            "BATCH NUMBER =  66\n",
            "BATCH NUMBER =  67\n",
            "BATCH NUMBER =  68\n",
            "BATCH NUMBER =  69\n",
            "BATCH NUMBER =  70\n",
            "BATCH NUMBER =  71\n",
            "BATCH NUMBER =  72\n",
            "BATCH NUMBER =  73\n",
            "BATCH NUMBER =  74\n",
            "BATCH NUMBER =  75\n",
            "BATCH NUMBER =  76\n",
            "BATCH NUMBER =  77\n",
            "BATCH NUMBER =  78\n",
            "BATCH NUMBER =  79\n",
            "BATCH NUMBER =  80\n",
            "BATCH NUMBER =  81\n",
            "BATCH NUMBER =  82\n",
            "BATCH NUMBER =  83\n",
            "BATCH NUMBER =  84\n",
            "BATCH NUMBER =  85\n",
            "BATCH NUMBER =  86\n",
            "BATCH NUMBER =  87\n",
            "BATCH NUMBER =  88\n",
            "BATCH NUMBER =  89\n",
            "BATCH NUMBER =  90\n",
            "BATCH NUMBER =  91\n",
            "BATCH NUMBER =  92\n",
            "BATCH NUMBER =  93\n",
            "BATCH NUMBER =  94\n",
            "BATCH NUMBER =  95\n",
            "BATCH NUMBER =  96\n",
            "BATCH NUMBER =  97\n",
            "BATCH NUMBER =  98\n",
            "BATCH NUMBER =  99\n",
            "BATCH NUMBER =  100\n",
            "BATCH NUMBER =  101\n",
            "BATCH NUMBER =  102\n",
            "BATCH NUMBER =  103\n",
            "BATCH NUMBER =  104\n",
            "BATCH NUMBER =  105\n",
            "BATCH NUMBER =  106\n",
            "BATCH NUMBER =  107\n",
            "BATCH NUMBER =  108\n",
            "BATCH NUMBER =  109\n",
            "BATCH NUMBER =  110\n",
            "BATCH NUMBER =  111\n",
            "BATCH NUMBER =  112\n",
            "BATCH NUMBER =  113\n",
            "BATCH NUMBER =  114\n",
            "BATCH NUMBER =  115\n",
            "BATCH NUMBER =  116\n",
            "BATCH NUMBER =  117\n",
            "BATCH NUMBER =  118\n",
            "BATCH NUMBER =  119\n",
            "BATCH NUMBER =  120\n",
            "BATCH NUMBER =  121\n",
            "BATCH NUMBER =  122\n",
            "BATCH NUMBER =  123\n",
            "BATCH NUMBER =  124\n",
            "BATCH NUMBER =  125\n",
            "BATCH NUMBER =  126\n",
            "BATCH NUMBER =  127\n",
            "BATCH NUMBER =  128\n",
            "BATCH NUMBER =  129\n",
            "BATCH NUMBER =  130\n",
            "BATCH NUMBER =  131\n",
            "BATCH NUMBER =  132\n",
            "BATCH NUMBER =  133\n",
            "BATCH NUMBER =  134\n",
            "BATCH NUMBER =  135\n",
            "BATCH NUMBER =  136\n",
            "BATCH NUMBER =  137\n",
            "BATCH NUMBER =  138\n",
            "BATCH NUMBER =  139\n",
            "BATCH NUMBER =  140\n",
            "BATCH NUMBER =  141\n",
            "BATCH NUMBER =  142\n",
            "BATCH NUMBER =  143\n",
            "BATCH NUMBER =  144\n",
            "BATCH NUMBER =  145\n",
            "BATCH NUMBER =  146\n",
            "BATCH NUMBER =  147\n",
            "BATCH NUMBER =  148\n",
            "BATCH NUMBER =  149\n",
            "BATCH NUMBER =  150\n",
            "BATCH NUMBER =  151\n",
            "BATCH NUMBER =  152\n",
            "BATCH NUMBER =  153\n",
            "BATCH NUMBER =  154\n",
            "BATCH NUMBER =  155\n",
            "BATCH NUMBER =  156\n",
            "BATCH NUMBER =  157\n",
            "BATCH NUMBER =  158\n",
            "BATCH NUMBER =  159\n",
            "BATCH NUMBER =  160\n",
            "BATCH NUMBER =  161\n",
            "BATCH NUMBER =  162\n",
            "train Loss: 0.5114 Acc: 0.8358\n",
            "STARTING ITERATION\n",
            "BATCH NUMBER =  0\n",
            "BATCH NUMBER =  1\n",
            "BATCH NUMBER =  2\n",
            "BATCH NUMBER =  3\n",
            "BATCH NUMBER =  4\n",
            "BATCH NUMBER =  5\n",
            "BATCH NUMBER =  6\n",
            "BATCH NUMBER =  7\n",
            "BATCH NUMBER =  8\n",
            "BATCH NUMBER =  9\n",
            "BATCH NUMBER =  10\n",
            "BATCH NUMBER =  11\n",
            "BATCH NUMBER =  12\n",
            "BATCH NUMBER =  13\n",
            "BATCH NUMBER =  14\n",
            "BATCH NUMBER =  15\n",
            "BATCH NUMBER =  16\n",
            "BATCH NUMBER =  17\n",
            "BATCH NUMBER =  18\n",
            "val Loss: 0.5363 Acc: 0.8263\n",
            "\n",
            "Training complete in 65m 17s\n",
            "Best val Acc: 0.826303\n"
          ]
        }
      ],
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ib3YKiuBV9iF",
        "outputId": "e6e11dc0-37c1-4d46-f1aa-8a844be1d59b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 128, 345, 3, 1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ceb618f7d338>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Pass inputs through the model with explicit data type conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-ceb618f7d338>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten all dimensions except batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 128, 345, 3, 1]"
          ]
        }
      ],
      "source": [
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 64 * 86, 128)  # Adjust input size based on your input dimensions\n",
        "        self.fc2 = nn.Linear(128, 13)  # 13 is the number of output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "inputs = inputs.squeeze(-1)  # Remove the last dimension\n",
        "inputs = inputs.squeeze(1)   # Remove the first dimension\n",
        "\n",
        "\n",
        "# Convert inputs to float\n",
        "inputs = inputs.float()\n",
        "\n",
        "# Convert bias tensor to the same data type as inputs\n",
        "for layer in model.modules():\n",
        "    if isinstance(layer, torch.nn.Conv2d):\n",
        "        layer.bias.data = layer.bias.data.float()\n",
        "\n",
        "\n",
        "# Define your model\n",
        "model = YourModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Move the model to the appropriate device (e.g., GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Pass inputs through the model with explicit data type conversion\n",
        "        outputs = model(inputs.float())\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Train Loss: {train_loss/len(train_dataloader.dataset):.4f}, \"\n",
        "          f\"Train Accuracy: {100*correct_train/total_train:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss/len(val_dataloader.dataset):.4f}, \"\n",
        "          f\"Val Accuracy: {100*correct_val/total_val:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}